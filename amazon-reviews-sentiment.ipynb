{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Копия блокнота \"Копия блокнота \"amazon_weak_baseline.ipynb\"\"",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdEHRLuOd8f8"
   },
   "source": [
    "# [Kaggle competition](https://www.kaggle.com/c/amazon-reviews-sentiment-2020-hse/leaderboard)\n",
    "\n",
    "Accuracy ~0.708"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WVy4pDn7A2LC"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "pd.set_option('max_colwidth', 400)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znD_fBMTde_y",
    "outputId": "9f120cee-a169-42c2-a5e5-24e32fd46160"
   },
   "source": [
    "# ! unzip amazon-reviews-sentiment-2020-hse.zip"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Archive:  amazon-reviews-sentiment-2020-hse.zip\n",
      "  inflating: eng_train_data.csv      \n",
      "  inflating: fr_text.csv             \n",
      "  inflating: sample_submission.csv   \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fkZDddt-U_Wg"
   },
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertPreTrainedModel, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "\n",
    "MAX_LEN = 128\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def create_attention_masks(encoded_sentences):\n",
    "    attention_masks = []\n",
    "    for sent in encoded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks\n",
    "\n",
    "\n",
    "def preprocessing(df, is_train=True):\n",
    "    sentences = df.sentence.values\n",
    "    if is_train:\n",
    "        labels = np.array([l for l in df.label.values])\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
    "\n",
    "    encoded_sentences = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "        encoded_sentences.append(encoded_sent)\n",
    "    encoded_sentences = pad_sequences(encoded_sentences, maxlen=MAX_LEN, dtype=\"long\",\n",
    "                                      value=0, truncating=\"post\", padding=\"post\")\n",
    "    if is_train:\n",
    "        return encoded_sentences, labels\n",
    "    return encoded_sentences\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    p = np.argmax(preds, axis=1).flatten()\n",
    "    l = labels.flatten()\n",
    "    return np.sum(p == l) / len(l)\n",
    "\n",
    "\n",
    "def run_train(model, train_dataloader, validation_dataloader, device, epochs, optimizer):\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        print(f'======== Epoch {e + 1} / {epochs} ========')\n",
    "        start_train_time = time.time()\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                elapsed = time.time() - start_train_time\n",
    "                print(f'{step}/{len(train_dataloader)} --> Time elapsed {elapsed}')\n",
    "\n",
    "            input_data = batch[0].to(device)\n",
    "            input_masks = batch[1].to(device)\n",
    "            input_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            out = model(input_data,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=input_masks,\n",
    "                        labels=input_labels)\n",
    "\n",
    "            loss = out[0]\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = total_loss / len(train_dataloader)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Training took {time.time() - start_train_time}\")\n",
    "\n",
    "        start_validation_time = time.time()\n",
    "        model.eval()\n",
    "        eval_loss, eval_acc = 0, 0\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            eval_data, eval_masks, eval_labels = batch\n",
    "            with torch.no_grad():\n",
    "                out = model(eval_data,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=eval_masks)\n",
    "            logits = out[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            eval_labels = eval_labels.to('cpu').numpy()\n",
    "            batch_acc = accuracy(logits, eval_labels)\n",
    "\n",
    "            eval_acc += batch_acc\n",
    "        print(f\"Accuracy: {eval_acc / (step + 1)}, Time elapsed: {time.time() - start_validation_time}\")\n",
    "    return losses, model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ylKW_EWDhmmB"
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "df = pd.read_csv(\"eng_train_data.csv\")\n",
    "df.columns = ['sentence', 'label']\n",
    "df_test = pd.read_csv(\"fr_text.csv\")\n",
    "df_test.columns = ['id', 'sentence']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZCzhfDDXkVMH"
   },
   "source": [
    "val = df.sample(int(len(df)*0.1))\n",
    "tr = df[~df.index.isin(val.index)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AgM2CRduU_Wi"
   },
   "source": [
    "train_encoded_sentences, train_labels = preprocessing(tr)\n",
    "train_attention_masks = create_attention_masks(train_encoded_sentences)\n",
    "\n",
    "test_encoded_sentences, test_labels = preprocessing(val)\n",
    "test_attention_masks = create_attention_masks(test_encoded_sentences)\n",
    "\n",
    "train_inputs = torch.tensor(train_encoded_sentences)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(test_encoded_sentences)\n",
    "validation_labels = torch.tensor(test_labels)\n",
    "validation_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "seed_val = 18\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Eb5E71Jj18x",
    "outputId": "95ac0c2c-1899-4ca4-f870-93cd27df0aee"
   },
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=3,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                    lr=3e-5,\n",
    "                    eps=1e-8,\n",
    "                    weight_decay=0.01\n",
    "                    )\n",
    "\n",
    "epochs = 1\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHj8D8UElPAR",
    "outputId": "3bb70509-65a5-4b7b-dcaf-47436c94d8bc"
   },
   "source": [
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "losses, model = run_train(model, train_dataloader, validation_dataloader, device, epochs, optimizer)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 1 ========\n",
      "0/844 --> Time elapsed 0.0022945404052734375\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:98: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "10/844 --> Time elapsed 6.925151348114014\n",
      "20/844 --> Time elapsed 13.86934781074524\n",
      "30/844 --> Time elapsed 20.934029817581177\n",
      "40/844 --> Time elapsed 28.11949610710144\n",
      "50/844 --> Time elapsed 35.356322288513184\n",
      "60/844 --> Time elapsed 42.650262117385864\n",
      "70/844 --> Time elapsed 49.84650111198425\n",
      "80/844 --> Time elapsed 56.97730875015259\n",
      "90/844 --> Time elapsed 64.02476119995117\n",
      "100/844 --> Time elapsed 71.02489924430847\n",
      "110/844 --> Time elapsed 78.0154767036438\n",
      "120/844 --> Time elapsed 84.98816752433777\n",
      "130/844 --> Time elapsed 91.99870920181274\n",
      "140/844 --> Time elapsed 99.00054264068604\n",
      "150/844 --> Time elapsed 106.06283140182495\n",
      "160/844 --> Time elapsed 113.1650800704956\n",
      "170/844 --> Time elapsed 120.28612470626831\n",
      "180/844 --> Time elapsed 127.39550185203552\n",
      "190/844 --> Time elapsed 134.48905634880066\n",
      "200/844 --> Time elapsed 141.5477590560913\n",
      "210/844 --> Time elapsed 148.60513877868652\n",
      "220/844 --> Time elapsed 155.6456425189972\n",
      "230/844 --> Time elapsed 162.70149731636047\n",
      "240/844 --> Time elapsed 169.74140763282776\n",
      "250/844 --> Time elapsed 176.79689168930054\n",
      "260/844 --> Time elapsed 183.8580436706543\n",
      "270/844 --> Time elapsed 190.90381002426147\n",
      "280/844 --> Time elapsed 197.9694540500641\n",
      "290/844 --> Time elapsed 205.03338384628296\n",
      "300/844 --> Time elapsed 212.1158151626587\n",
      "310/844 --> Time elapsed 219.1888210773468\n",
      "320/844 --> Time elapsed 226.26384735107422\n",
      "330/844 --> Time elapsed 233.34844732284546\n",
      "340/844 --> Time elapsed 240.4176948070526\n",
      "350/844 --> Time elapsed 247.516371011734\n",
      "360/844 --> Time elapsed 254.59381127357483\n",
      "370/844 --> Time elapsed 261.68424105644226\n",
      "380/844 --> Time elapsed 268.76412057876587\n",
      "390/844 --> Time elapsed 275.8534438610077\n",
      "400/844 --> Time elapsed 282.94189620018005\n",
      "410/844 --> Time elapsed 290.02731919288635\n",
      "420/844 --> Time elapsed 297.11601543426514\n",
      "430/844 --> Time elapsed 304.2043960094452\n",
      "440/844 --> Time elapsed 311.2888250350952\n",
      "450/844 --> Time elapsed 318.37944078445435\n",
      "460/844 --> Time elapsed 325.4572789669037\n",
      "470/844 --> Time elapsed 332.54400992393494\n",
      "480/844 --> Time elapsed 339.635457277298\n",
      "490/844 --> Time elapsed 346.73693776130676\n",
      "500/844 --> Time elapsed 353.8056457042694\n",
      "510/844 --> Time elapsed 360.8734426498413\n",
      "520/844 --> Time elapsed 367.93239879608154\n",
      "530/844 --> Time elapsed 374.99865102767944\n",
      "540/844 --> Time elapsed 382.072304725647\n",
      "550/844 --> Time elapsed 389.12252593040466\n",
      "560/844 --> Time elapsed 396.1857030391693\n",
      "570/844 --> Time elapsed 403.23666501045227\n",
      "580/844 --> Time elapsed 410.2859139442444\n",
      "590/844 --> Time elapsed 417.36139607429504\n",
      "600/844 --> Time elapsed 424.42185401916504\n",
      "610/844 --> Time elapsed 431.4721026420593\n",
      "620/844 --> Time elapsed 438.5359933376312\n",
      "630/844 --> Time elapsed 445.60955262184143\n",
      "640/844 --> Time elapsed 452.66766119003296\n",
      "650/844 --> Time elapsed 459.73082542419434\n",
      "660/844 --> Time elapsed 466.80907702445984\n",
      "670/844 --> Time elapsed 473.8984582424164\n",
      "680/844 --> Time elapsed 480.9823651313782\n",
      "690/844 --> Time elapsed 488.06927371025085\n",
      "700/844 --> Time elapsed 495.1384744644165\n",
      "710/844 --> Time elapsed 502.20914936065674\n",
      "720/844 --> Time elapsed 509.26461148262024\n",
      "730/844 --> Time elapsed 516.299295425415\n",
      "740/844 --> Time elapsed 523.3588442802429\n",
      "750/844 --> Time elapsed 530.4356753826141\n",
      "760/844 --> Time elapsed 537.5164971351624\n",
      "770/844 --> Time elapsed 544.6084232330322\n",
      "780/844 --> Time elapsed 551.707622051239\n",
      "790/844 --> Time elapsed 558.8028156757355\n",
      "800/844 --> Time elapsed 565.8879432678223\n",
      "810/844 --> Time elapsed 572.9750604629517\n",
      "820/844 --> Time elapsed 580.0566504001617\n",
      "830/844 --> Time elapsed 587.1041095256805\n",
      "840/844 --> Time elapsed 594.1684358119965\n",
      "Training took 596.8153870105743\n",
      "Accuracy: 0.8090598982423682, Time elapsed: 22.11089062690735\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gq3aMnWOleUX"
   },
   "source": [
    "def run_evaluation(df_test, model, device):\n",
    "    test_encoded_sentences = preprocessing(df_test, False)\n",
    "    test_attention_masks = create_attention_masks(test_encoded_sentences)\n",
    "\n",
    "    test_inputs = torch.tensor(test_encoded_sentences)\n",
    "    test_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "    test_data = TensorDataset(test_inputs, test_masks)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_acc = 0, 0\n",
    "    res = []\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        eval_data, eval_masks = batch\n",
    "        with torch.no_grad():\n",
    "            out = model(eval_data,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=eval_masks)\n",
    "        logits = out[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        res.extend(logits)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "labels_fr = run_evaluation(df_test,  model, device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qKB3hv1Bu_7l"
   },
   "source": [
    "labels_fr_concat = [np.argmax(i) for i in labels_fr]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CcT1a8I1vphO"
   },
   "source": [
    "df_test['class'] = labels_fr_concat\n",
    "df_test.head()\n",
    "df_test[['id', 'class']].to_csv('lr_solution_bert.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}